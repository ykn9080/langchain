{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# |Good parts:\n",
        "# |1. The code imports necessary modules and classes, indicating that it follows modular programming principles.\n",
        "# |2. The code uses a callback handler, `StreamingStdOutCallbackHandler`, which suggests that it can handle streaming output efficiently.\n",
        "# |3. The code initializes an instance of the `ChatOpenAI` class, passing appropriate parameters such as temperature, max_tokens, and model_name.\n",
        "# |4. The code sets the streaming parameter to True, indicating that it can handle streaming input.\n",
        "# |5. The code calls the `predict` method of the `ChatOpenAI` instance, passing a prompt to generate a response.\n",
        "# |\n",
        "# |Bad parts:\n",
        "# |1. The code imports `apikey` module but does not use it correctly. It calls `apikey.getOpenai()` without assigning the returned value to any variable or using it in the subsequent code. This suggests that the code may not be functioning as intended.\n",
        "# |2. The code does not handle any exceptions or errors that may occur during the execution. It would be better to include appropriate error handling mechanisms to ensure the code's robustness.\n",
        "# |3. The code does not include any comments or documentation to explain the purpose and functionality of the different parts. Adding comments would improve code readability and maintainability.\n",
        "# |4. The code does not follow consistent naming conventions. For example, `llm` is not a descriptive variable name and does not follow the recommended lowercase_with_underscores naming style.\n",
        "# |\n",
        "# |Overall, the code seems to have some issues related to the usage of the `apikey` module and lacks proper error handling and documentation.\n",
        "# |\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from api import apikey\n",
        "\n",
        "apikey.getOpenai()\n",
        "\n",
        "# \uac1d\uccb4 \uc0dd\uc131\n",
        "llm = ChatOpenAI(temperature=0,               # \ucc3d\uc758\uc131 (0.0 ~ 2.0)\n",
        "                 max_tokens=2048,             # \ucd5c\ub300 \ud1a0\ud070\uc218\n",
        "                 model_name='gpt-3.5-turbo',  # \ubaa8\ub378\uba85\n",
        "                 streaming=True,\n",
        "                 callbacks=[StreamingStdOutCallbackHandler()]\n",
        "                 )\n",
        "llm.predict(\"\ub300\ud55c\ubbfc\uad6d \uc81c 3\uc758 \ub3c4\uc2dc\ub294?\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}